{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raminass/AI-Wrokshop/blob/main/workshop_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1yt2h6gkI1u"
      },
      "source": [
        "# Building and Deploying AI Microservices: A Practical Guide\n",
        "\n",
        "This tutorial will guide you through building a practical AI system using a microservices architecture. Rather than covering theoretical concepts alone, we'll build a real-world AI application step-by-step. By the end, you'll have created a fully functional AI system with multiple services working together.\n",
        "\n",
        "We'll build a text analysis platform with three core services:\n",
        "1. **Sentiment Analysis Service** (using HuggingFace)\n",
        "2. **Text Generation Service** (using OpenAI API)\n",
        "3. **API Gateway** (to coordinate requests)\n",
        "\n",
        "This architecture demonstrates key microservices principles while creating something useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hROMyptdkI1v"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Introduction to AI Microservices](#intro)\n",
        "2. [Building AI Services with Python](#building)\n",
        "3. [Containerizing AI Applications with Docker](#docker)\n",
        "4. [Local Model Integration (HuggingFace)](#huggingface)\n",
        "5. [External API Integration (OpenAI)](#openai)\n",
        "6. [Deployment with Docker Compose](#deployment)\n",
        "7. [Monitoring and Observability](#monitoring)\n",
        "8. [Performance Optimization](#performance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WmlBD4pkI1v"
      },
      "source": [
        "<a id=\"intro\"></a>\n",
        "## 1. Introduction to AI Microservices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-tR2GlFkI1v"
      },
      "source": [
        "**Monolithic Architecture**\n",
        "- Single, unified codebase\n",
        "- All components deployed together\n",
        "- Shared database\n",
        "- Simple to develop initially\n",
        "- Challenges with scaling and maintenance\n",
        "\n",
        "**Microservices Architecture**\n",
        "- Collection of small, independent services\n",
        "- Each service focused on specific business capability\n",
        "- Decentralized data management\n",
        "- Independent deployment\n",
        "- Technology diversity\n",
        "\n",
        "\n",
        "**For AI applications, typical services include:**\n",
        "- API Gateway (entry point for all clients)\n",
        "- Model Serving Service\n",
        "- Feature Processing Service\n",
        "- Data Storage Service\n",
        "- Monitoring Service\n",
        "- More ...\n",
        "\n",
        "### Communication Patterns\n",
        "\n",
        "Two main approaches:\n",
        "\n",
        "**Synchronous (REST/HTTP):**\n",
        "```\n",
        "Client → Request → Server → Response → Client\n",
        "```\n",
        "\n",
        "**Asynchronous (Message Queue):**\n",
        "```\n",
        "Producer → Message → Queue → Message → Consumer\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHNjBSUGkI1w"
      },
      "source": [
        "#### API Gateway Pattern\n",
        "- Single entry point for all clients\n",
        "- Routing, composition, protocol translation\n",
        "- Security, monitoring, rate limiting\n",
        "- Reduces client complexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qwS3FojkI1w"
      },
      "source": [
        "![Communication Patterns Diagram](https://qentelli.com/sites/default/files/inline-images/API-gateway-pattern.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faYGoRBikI1w"
      },
      "source": [
        "### Real-World Microservices Examples in AI\n",
        "\n",
        "1. **Netflix**: Recommendation engine as a service\n",
        "2. **Uber**: ML-based ETA prediction and surge pricing\n",
        "3. **Spotify**: Music recommendation engine\n",
        "4. **Amazon**: Product recommendation services\n",
        "5. **OpenAI**: API services for various AI models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0Jg_WVrkI1w"
      },
      "source": [
        "<a id=\"building\"></a>\n",
        "## 2. Building AI Services with Python\n",
        "### Sample Flask API Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBt9dyTAkI1w"
      },
      "outputs": [],
      "source": [
        "rom flask import Flask, request, jsonify\n",
        "import time\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\"status\": \"healthy\"}), 200\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.json\n",
        "\n",
        "    if not data or 'text' not in data:\n",
        "        return jsonify({\"error\": \"No text provided\"}), 400\n",
        "\n",
        "    # Your ML logic here\n",
        "    result = {\"prediction\": \"positive\", \"confidence\": 0.92}\n",
        "\n",
        "    return jsonify(result)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2gBJ85dkI1x"
      },
      "source": [
        "### Alternative: FastAPI for Higher Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb4XltqMkI1x"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class TextInput(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"healthy\"}\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(input_data: TextInput):\n",
        "    # Your ML logic here\n",
        "    result = {\"prediction\": \"positive\", \"confidence\": 0.92}\n",
        "    return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvfuFGPQkI1x"
      },
      "source": [
        "### API Design Best Practices\n",
        "\n",
        "1. **API First Development**: Design APIs before implementation\n",
        "2. **Versioning**: Plan for evolution and backward compatibility\n",
        "3. **Documentation**: OpenAPI (Swagger) specifications\n",
        "4. **Security**: Authentication, authorization, encryption\n",
        "5. **Idempotency**: Safe retries for failed requests\n",
        "6. **Rate Limiting**: Protect services from overload\n",
        "7. **Status Codes**: Proper use of HTTP status codes\n",
        "8. **Pagination**: Efficient handling of large collections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfLmNbMXkI1x"
      },
      "source": [
        "<a id=\"docker\"></a>\n",
        "## 3. Containerizing AI Applications with Docker\n",
        "\n",
        "Docker provides consistent environments from development to production.\n",
        "### Installation Steps\n",
        "\n",
        "#### macOS Installation\n",
        "```bash\n",
        "# Install Docker Desktop for Mac using Homebrew\n",
        "brew update\n",
        "brew install --cask docker\n",
        "\n",
        "# Start Docker Desktop from Applications folder\n",
        "# Or run the command:\n",
        "open -a Docker\n",
        "```\n",
        "#### Windows Installation\n",
        "```bash\n",
        "# Download Docker Desktop for Windows from:\n",
        "# https://www.docker.com/products/docker-desktop/\n",
        "\n",
        "# Follow the installation wizard\n",
        "# Enable WSL 2 if prompted\n",
        "```\n",
        "\n",
        "### Verify installation\n",
        "```bash\n",
        "# Verify installation\n",
        "docker --version\n",
        "docker run hello-world\n",
        "```\n",
        "\n",
        "### Test installation\n",
        "```bash\n",
        "docker run hello-world\n",
        "```\n",
        "If successful, you'll see a message indicating that your Docker installation is working correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQL1twkEkI1x"
      },
      "source": [
        "![Docker Flow](https://ucarecdn.com/2f29c783-13a6-4370-8260-0116340242e5/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eXHqEFckI1x"
      },
      "source": [
        "### Project Structure\n",
        "\n",
        "```bash\n",
        "text-analysis-platform/\n",
        "├── sentiment-service/\n",
        "│   ├── app.py\n",
        "│   ├── Dockerfile\n",
        "│   ├── requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIsdm_lfkI1x"
      },
      "source": [
        "\n",
        "### Basic Dockerfile\n",
        "```bash\n",
        "FROM python:3.9-slim\n",
        "WORKDIR /app\n",
        "# Install required system dependencies for PyTorch and transformers\n",
        "RUN apt-get update && apt-get install -y \\\n",
        "    build-essential \\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "# Copy requirements file first for better caching\n",
        "COPY requirements.txt .\n",
        "# Install Python dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "# Copy application code\n",
        "COPY . .\n",
        "# Expose the port the app runs on\n",
        "EXPOSE 5005\n",
        "# Command to run the application\n",
        "CMD [\"python\", \"app.py\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOdYY0mvkI1y"
      },
      "source": [
        "\n",
        "### requirements.txt\n",
        "\n",
        "```bash\n",
        "flask==2.0.1\n",
        "transformers==4.21.0\n",
        "torch==1.13.1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5AlWG_ekI1y"
      },
      "source": [
        "### Building and Running the Sentiment Analysis Container\n",
        "\n",
        "```bash\n",
        "# Build an image\n",
        "docker build -t sentiment-analyzer .\n",
        "# Run a container\n",
        "docker run -d -p 5005:5005 --name sentiment-api sentiment-analyzer\n",
        "# Check if the container is running\n",
        "docker ps\n",
        "# View logs from the container\n",
        "docker logs sentiment-api\n",
        "# Test the API using curl\n",
        "curl -X POST -H \"Content-Type: application/json\" -d '{\"text\":\"I love this product!\"}' http://localhost:5005/analyze\n",
        "# Stop and remove the container\n",
        "docker stop sentiment-api\n",
        "docker rm sentiment-api\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfxx1RRTkI1y"
      },
      "source": [
        "### Components of Production AI Systems\n",
        "\n",
        "1. **Data Collection & Storage**\n",
        "   - Data ingestion pipelines\n",
        "   - Data lakes and warehouses\n",
        "   - Feature stores\n",
        "\n",
        "2. **Model Development**\n",
        "   - Experimentation environments\n",
        "   - Version control for ML models\n",
        "   - Reproducible builds\n",
        "\n",
        "3. **Model Serving**\n",
        "   - Inference services\n",
        "   - API design for ML endpoints\n",
        "   - Batch vs real-time processing\n",
        "\n",
        "4. **Model Monitoring**\n",
        "   - Performance tracking\n",
        "   - Drift detection\n",
        "   - Alerting systems\n",
        "\n",
        "5. **Feedback Loops**\n",
        "   - User feedback collection\n",
        "   - A/B testing infrastructure\n",
        "   - Model retraining pipelines\n",
        "\n",
        "### Model Serving Architectures\n",
        "\n",
        "#### Model-as-a-Service\n",
        "- Direct API calls to model\n",
        "- Dedicated inference servers\n",
        "- Examples: TensorFlow Serving, Triton Inference Server\n",
        "\n",
        "#### Embedded Models\n",
        "- Models deployed within application services\n",
        "- Lower latency, higher coupling\n",
        "- Suitable for smaller models or edge deployment\n",
        "\n",
        "#### Batch Inference\n",
        "- Scheduled batch processing of data\n",
        "- Higher throughput, not real-time\n",
        "- Cost-efficient for non-time-sensitive applications\n",
        "\n",
        "#### Hybrid Approaches\n",
        "- Pre-computed results with real-time fallback\n",
        "- Cached inferences with live updates\n",
        "- Best of both worlds for certain applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWZhKKqRkI1y"
      },
      "source": [
        "<a id=\"huggingface\"></a>\n",
        "## 4. HuggingFace Model Integration\n",
        "\n",
        "HuggingFace provides easy access to thousands of pre-trained models.\n",
        "### Setting Up a Sentiment Analysis Service\n",
        "\n",
        "```bash\n",
        "pip install transformers torch flask\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYcOBmnKkI1y"
      },
      "outputs": [],
      "source": [
        "# Create a simple Flask app with HuggingFace integration\n",
        "#sentiment_analyzer.py\n",
        "from flask import Flask, request, jsonify\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "print(\"Loading sentiment analysis model...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "print(f\"Model loaded in {load_time:.2f} seconds!\")\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\"status\": \"healthy\"}), 200\n",
        "\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "def analyze_text():\n",
        "    data = request.json\n",
        "    if not data or 'text' not in data:\n",
        "        return jsonify({\"error\": \"No text provided\"}), 400\n",
        "\n",
        "    text = data['text']\n",
        "\n",
        "    # Perform sentiment analysis using HuggingFace\n",
        "    result = sentiment_analyzer(text)\n",
        "\n",
        "    # Format and return the result\n",
        "    sentiment = {\n",
        "        \"label\": result[0]['label'],\n",
        "        \"score\": float(result[0]['score'])\n",
        "    }\n",
        "\n",
        "    return jsonify({\"sentiment\": sentiment})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0OkUTQkkI1y"
      },
      "source": [
        "### Dockerfile for HuggingFace Service\n",
        "\n",
        "```bash\n",
        "FROM python:3.9-slim\n",
        "WORKDIR /app\n",
        "# Install dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "# Create directory for pre-downloaded models\n",
        "RUN mkdir -p /app/models\n",
        "# Script to download model at build time\n",
        "COPY download_model.py .\n",
        "RUN python download_model.py\n",
        "# Copy application code\n",
        "COPY . .\n",
        "# Expose port\n",
        "EXPOSE 8000\n",
        "# Run the application\n",
        "CMD [\"python\", \"sentiment_analyzer.py\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGtZgUozkI1y"
      },
      "source": [
        "### Model Download Script\n",
        "\n",
        "This script pre-downloads the model during the Docker build phase, which significantly improves startup time for your container. Without this, the model would be downloaded when the container first starts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18tH6FbukI1y"
      },
      "outputs": [],
      "source": [
        "# download_model.py\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import os\n",
        "\n",
        "# Specify the model you want to use (same as in app.py)\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "cache_dir = \"/app/models\"\n",
        "\n",
        "# Create cache directory if it doesn't exist\n",
        "os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Downloading model: {model_name}\")\n",
        "\n",
        "# Download and cache the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=cache_dir)\n",
        "\n",
        "# Save model info to verify it's ready\n",
        "with open(os.path.join(cache_dir, \"model_info.txt\"), \"w\") as f:\n",
        "    f.write(f\"Model: {model_name}\\n\")\n",
        "    f.write(f\"Tokenizer vocabulary size: {len(tokenizer)}\\n\")\n",
        "    f.write(f\"Model parameters: {model.num_parameters()}\\n\")\n",
        "\n",
        "print(\"Model downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2be0H0DkI1y"
      },
      "source": [
        "To use this pre-download approach, update your Dockerfile like this:\n",
        "\n",
        "```bash\n",
        "FROM python:3.9-slim\n",
        "WORKDIR /app\n",
        "# Install required system dependencies\n",
        "RUN apt-get update && apt-get install -y \\\n",
        "    build-essential \\\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "# Copy requirements file first for better caching\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "# Create directory for pre-downloaded models\n",
        "RUN mkdir -p /app/models\n",
        "# Copy and run model download script BEFORE copying all app code\n",
        "# This creates a separate layer that won't be rebuilt unless the script changes\n",
        "COPY download_model.py .\n",
        "RUN python download_model.py\n",
        "# Now copy the rest of the application\n",
        "COPY . .\n",
        "# Update app.py to use the cached model (if needed)\n",
        "# You would modify app.py to look for models in /app/models\n",
        "EXPOSE 5005\n",
        "CMD [\"python\", \"app.py\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cim77KsRkI1y"
      },
      "source": [
        "### Additional Optimizations for Production\n",
        "\n",
        "1. **Model Quantization**: Convert the model to FP16 for faster inference and smaller size\n",
        "```python\n",
        "# After loading the model\n",
        "model = model.half()  # Convert to FP16\n",
        "```\n",
        "\n",
        "2. **Batch Processing**: Add a batch endpoint for processing multiple texts at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg-l4N9OkI1z"
      },
      "outputs": [],
      "source": [
        "@app.route('/analyze-batch', methods=['POST'])\n",
        "def analyze_batch():\n",
        "    data = request.json\n",
        "    if not data or 'texts' not in data:\n",
        "        return jsonify({\"error\": \"No texts provided\"}), 400\n",
        "\n",
        "    texts = data['texts']\n",
        "    results = sentiment_analyzer(texts)\n",
        "\n",
        "    formatted_results = []\n",
        "    for i, result in enumerate(results):\n",
        "        formatted_results.append({\n",
        "            \"text\": texts[i],\n",
        "            \"label\": result['label'],\n",
        "            \"score\": float(result['score'])\n",
        "        })\n",
        "\n",
        "    return jsonify({\"results\": formatted_results})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8XUZb0FkI1z"
      },
      "source": [
        "3. **Turn off debug mode in production**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqrEMLKpkI1z"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    app.run(debug=False, host='0.0.0.0', port=5005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9hyIZ8xkI1z"
      },
      "source": [
        "<a id=\"openai\"></a>\n",
        "## 5. External API Integration (OpenAI)\n",
        "\n",
        "Using external AI APIs can be simpler than hosting your own models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysHwQb4zkI1z"
      },
      "outputs": [],
      "source": [
        "#penai_service.py\n",
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "import openai\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\"status\": \"healthy\"}), 200\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate_text():\n",
        "    data = request.json\n",
        "    if not data or 'prompt' not in data:\n",
        "        return jsonify({\"error\": \"No prompt provided\"}), 400\n",
        "\n",
        "    prompt = data['prompt']\n",
        "    max_tokens = data.get('max_tokens', 100)\n",
        "    temperature = data.get('temperature', 0.7)\n",
        "\n",
        "    try:\n",
        "        # Call OpenAI API\n",
        "        completion = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-instruct\",\n",
        "            prompt=prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "\n",
        "        # Return the generated text\n",
        "        return jsonify({\n",
        "            \"generated_text\": completion.choices[0].text.strip(),\n",
        "            \"usage\": completion.usage\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYE2SN46kI1z"
      },
      "source": [
        "\n",
        "### Security Best Practices for API Keys\n",
        "\n",
        "NEVER include API keys in your code or Dockerfile\n",
        "Use environment variables instead. During development:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c5f8NZhkI1z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"  # For testing only\n",
        "# In production: Pass as environment variable in Docker\n",
        "# docker run -e OPENAI_API_KEY=your-key-here openai-service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTWDRclIkI1z"
      },
      "source": [
        "<a id=\"deployment\"></a>\n",
        "## 6. Deployment with Docker Compose\n",
        "\n",
        "Docker Compose simplifies multi-container deployments.\n",
        "\n",
        "### Simple Docker Compose File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yyJb38QkI1z"
      },
      "source": [
        "```bash\n",
        "#docker-compose.yml\n",
        "version: '3'\n",
        "services:\n",
        "  frontend:\n",
        "    build: ./frontend-service\n",
        "    ports:\n",
        "      - \"3000:3000\"\n",
        "    depends_on:\n",
        "      - api-gateway\n",
        "    networks:\n",
        "      - app-network\n",
        "\n",
        "  api-gateway:\n",
        "    build: ./api-gateway\n",
        "    ports:\n",
        "      - \"8000:8000\"\n",
        "    depends_on:\n",
        "      - sentiment-service\n",
        "      - openai-service\n",
        "    networks:\n",
        "      - app-network\n",
        "    environment:\n",
        "      - SENTIMENT_SERVICE_URL=http://sentiment-service:8001\n",
        "      - OPENAI_SERVICE_URL=http://openai-service:8002\n",
        "\n",
        "  sentiment-service:\n",
        "    build: ./sentiment-service\n",
        "    volumes:\n",
        "      - ./models:/app/models\n",
        "    networks:\n",
        "      - app-network\n",
        "\n",
        "  openai-service:\n",
        "    build: ./openai-service\n",
        "    networks:\n",
        "      - app-network\n",
        "    environment:\n",
        "      - OPENAI_API_KEY=${OPENAI_API_KEY}\n",
        "\n",
        "  mongodb:\n",
        "    image: mongo:latest\n",
        "    volumes:\n",
        "      - mongo-data:/data/db\n",
        "    networks:\n",
        "      - app-network\n",
        "\n",
        "networks:\n",
        "  app-network:\n",
        "    driver: bridge\n",
        "\n",
        "volumes:\n",
        "  mongo-data:\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t1VcZq4kI1z"
      },
      "source": [
        "### Running the Application\n",
        "\n",
        "```bash\n",
        "# Start all services\n",
        "docker-compose up -d\n",
        "# View running containers\n",
        "docker-compose ps\n",
        "# View logs from a specific service\n",
        "docker-compose logs sentiment-service\n",
        "# Stop all services\n",
        "docker-compose down\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5zQ-UhkI10"
      },
      "source": [
        "\n",
        "### Adding Prometheus and Grafana to Docker Compose\n",
        "\n",
        "```bash\n",
        "  prometheus:\n",
        "    image: prom/prometheus\n",
        "    ports:\n",
        "      - \"9090:9090\"\n",
        "    volumes:\n",
        "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
        "    networks:\n",
        "      - app-network\n",
        "\n",
        "  grafana:\n",
        "    image: grafana/grafana\n",
        "    ports:\n",
        "      - \"3001:3000\"\n",
        "    depends_on:\n",
        "      - prometheus\n",
        "    networks:\n",
        "      - app-network\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mQ1Pr5UkI10"
      },
      "source": [
        "### Basic Prometheus Configuration\n",
        "```bash\n",
        "# prometheus.yml\n",
        "global:\n",
        "  scrape_interval: 15s\n",
        "\n",
        "scrape_configs:\n",
        "  - job_name: 'sentiment-service'\n",
        "    static_configs:\n",
        "      - targets: ['sentiment-service:8001']\n",
        "  \n",
        "  - job_name: 'api-gateway'\n",
        "    static_configs:\n",
        "      - targets: ['api-gateway:8001']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLezL1BIkI10"
      },
      "source": [
        "### Example structure\n",
        "\n",
        "```bash\n",
        "text-analysis-platform/\n",
        "├── sentiment-service/\n",
        "├── openai-service/\n",
        "├── api-gateway/\n",
        "├── frontend/\n",
        "│   ├── app.py\n",
        "│   ├── templates/\n",
        "│   │   └── index.html\n",
        "│   ├── Dockerfile\n",
        "│   └── requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWe2sXR8kI15"
      },
      "source": [
        "<a id=\"performance\"></a>\n",
        "## 8. Performance Optimization\n",
        "\n",
        "### Model Optimization Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC0kj3pNkI15"
      },
      "outputs": [],
      "source": [
        "# 1. Use smaller models when possible\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"  # Distilled version\n",
        "\n",
        "# 2. Implement request batching\n",
        "texts = [\"I love this product\", \"This is terrible\", \"Not bad at all\"]\n",
        "results = sentiment_analyzer(texts)  # More efficient than individual calls\n",
        "\n",
        "# 3. Model quantization for faster inference\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model = model.half()  # Convert to FP16 for faster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hidqjFUkI15"
      },
      "source": [
        "### Request Batching Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W55V2iokI15"
      },
      "outputs": [],
      "source": [
        "@app.route('/analyze-batch', methods=['POST'])\n",
        "def analyze_batch():\n",
        "    data = request.json\n",
        "    if not data or 'texts' not in data:\n",
        "        return jsonify({\"error\": \"No texts provided\"}), 400\n",
        "\n",
        "    texts = data['texts']\n",
        "    if not texts or len(texts) > 100:  # Limit batch size\n",
        "        return jsonify({\"error\": \"Invalid batch size (1-100)\"}), 400\n",
        "\n",
        "    # Process batch\n",
        "    results = sentiment_analyzer(texts)\n",
        "\n",
        "    # Format results\n",
        "    formatted_results = []\n",
        "    for i, result in enumerate(results):\n",
        "        formatted_results.append({\n",
        "            \"text\": texts[i],\n",
        "            \"label\": result['label'],\n",
        "            \"score\": float(result['score'])\n",
        "        })\n",
        "\n",
        "    return jsonify({\"results\": formatted_results})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1Yv5W5ckI15"
      },
      "source": [
        "### Implementing Result Caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBLsqJJqkI15"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=1000)  # Cache up to 1000 results\n",
        "def get_sentiment(text):\n",
        "    return sentiment_analyzer(text)[0]\n",
        "\n",
        "@app.route('/analyze-cached', methods=['POST'])\n",
        "def analyze_text_cached():\n",
        "    data = request.json\n",
        "    if not data or 'text' not in data:\n",
        "        return jsonify({\"error\": \"No text provided\"}), 400\n",
        "\n",
        "    text = data['text']\n",
        "\n",
        "    # Get result (will use cache if available)\n",
        "    result = get_sentiment(text)\n",
        "\n",
        "    sentiment = {\n",
        "        \"label\": result['label'],\n",
        "        \"score\": float(result['score'])\n",
        "    }\n",
        "\n",
        "    return jsonify({\"sentiment\": sentiment})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Uq-EiLkI15"
      },
      "source": [
        "\n",
        "## Conclusion\n",
        "\n",
        "This notebook has covered the practical aspects of building, containerizing, and deploying AI microservices. The techniques shown here can be adapted and expanded for a wide variety of AI applications.\n",
        "\n",
        "### Useful Links\n",
        "\n",
        "#### 1. Docker and Containerization\n",
        "* Tutorial: [Docker for Beginners](https://www.youtube.com/watch?v=fqMOX6JJhGo) by FreeCodeCamp\n",
        "\n",
        "#### 2. Kubernetes for Orchestration\n",
        "* Course: [Kubernetes Essentials](https://www.udacity.com/course/kubernetes-essentials--ud615) on Udacity\n",
        "\n",
        "#### 3. Microservices Architecture\n",
        "* Video Series: [Microservices Architecture](https://www.youtube.com/playlist?list=PLkQkbY7JNJuDqCFncFdTzGm6cRYCF-kZO) by TECH DOSE\n",
        "\n",
        "#### 4. CI/CD Pipelines\n",
        "* Tutorial: [GitHub Actions Tutorial](https://www.youtube.com/watch?v=R8_veQiYBjI) by TechWorld with Nana\n",
        "\n",
        "#### 5. API Design and Development\n",
        "* Course: [API Development with Flask](https://www.codecademy.com/learn/paths/create-rest-apis-with-flask-and-python) on Codecademy\n",
        "\n",
        "#### 6. Machine Learning Model Deployment\n",
        "* Guide: [Deploying ML Models](https://www.tensorflow.org/tfx/guide/serving) with TensorFlow Serving\n",
        "* Tutorial: [Machine Learning Model deployment with FastAPI, Streamlit and Docker](https://medium.com/latinxinai/fastapi-and-streamlit-app-with-docker-compose-e4d18d78d61d)\n",
        "\n",
        "#### 7. Model Monitoring\n",
        "* [Weights & Biases Documentation](https://docs.wandb.ai/) - Industry standard tool for experiment tracking and model monitoring\n",
        "* Tutorial: [Model Monitoring with W&B](https://wandb.ai/site/model-monitoring) - Learn how to track model performance in production\n",
        "* Guide: [Integrating W&B with ML Services](https://docs.wandb.ai/guides/integrations) - Step-by-step integration examples\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "software_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}